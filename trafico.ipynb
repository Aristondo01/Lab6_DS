{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorio 6\n",
    "\n",
    "#### 20880 Sebastian Aristondo\n",
    "#### 20293 Daniel Gonzalez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "from nltk.corpus import opinion_lexicon\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "import datetime\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import pipeline\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('traficogt.csv', sep=',')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Limpieza y preprocesamiento de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El dataset tiene los datos crudos, por lo que debemos de limpiarlos y preprocesarlos para poder trabajar con ellos. Para esto, se utilizo la libreria pandas para poder leer el archivo csv y poder trabajar con el. Luego utilizaremos varias funciones de nltk para poder limpiar los datos y dejarlos listos para poder trabajar con ellos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(rawContent):\n",
    "    url_pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    return re.sub(url_pattern, '', rawContent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se realizaron tres acciones iniciales para limpiar y preprocesar datos. Primero se eliminó cualquier URL de los datos usando una expresión regular. También se mantuvieron solamente los caracteres que estuvieran de la a a la z, mayúsculas o minúsculas. Esto quiere decir que se quitó cualquier caracter como \"#\" o \"@\" y signos de puntuación. Por otra parte, se pasaron todas las palabras a minúsculas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_tildes(rawContent):\n",
    "    rawContent = rawContent.replace('á', 'a')\n",
    "    rawContent = rawContent.replace('é', 'e')\n",
    "    rawContent = rawContent.replace('í', 'i')\n",
    "    rawContent = rawContent.replace('ó', 'o')\n",
    "    rawContent = rawContent.replace('ú', 'u')\n",
    "    return rawContent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['rawContent'] = data['rawContent'].apply(remove_urls)\n",
    "data['rawContent'] = data['rawContent'].apply(lambda x: x.lower())\n",
    "data['rawContent'] = data['rawContent'].apply(remove_tildes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['rawContent'] = data['rawContent'].str.replace('[^a-zA-Z0-9]', ' ', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se removieron URLS y se quitaron caracteres especiales. También se pasaron todos los tweets a minúsculas y se removieron las tildes, para poder tener una forma estandarizada de los símbolos de los tweets en el dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(sentence):\n",
    "    stop_words = set(stopwords.words('spanish'))  \n",
    "    words = sentence.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    new_sentence = ' '.join(filtered_words)\n",
    "    return new_sentence\n",
    "\n",
    "data['rawContent_clean'] = data['rawContent'].apply(remove_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se eliminaron las stop words en español que pudiera tener el dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['rawContent_clean'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_words(words):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return lemmatized_words\n",
    "\n",
    "data['rawContent_lemmatized'] = data['rawContent_clean'].apply(lambda x: lemmatize_words(x.split()))\n",
    "data['rawContent_lemmatized_text'] = data['rawContent_lemmatized'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el objetivo de analizar de una manera más sencilla los tweets los lematizaremos para poder obtener palabras clave como zona y lluvia.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['rawContent_lemmatized'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['rawContent_lemmatized_text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(\"number\", axis=1)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_non_spanish_tweets(texto):\n",
    "    try:\n",
    "        return detect(texto) == 'es'\n",
    "    except:\n",
    "        # Si no se puede detectar el idioma, se asume que no es español\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['rawContent'].apply(delete_non_spanish_tweets)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Análisis exploratorio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nas_coordenadas = data['coordinates'].isna().sum()\n",
    "print('Porcentaje de tweets sin coordenadas: ', nas_coordenadas/data.shape[0]*100)\n",
    "\n",
    "print(\"Cantidad de tweets con coordenadas: \", data.shape[0] - nas_coordenadas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede observar, casi todos los tweets no tienen coordenadas, por lo tanto, no es posible realizar un análisis de tweets por ubicación para determinar que áreas tienen más tráfico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coincidencias = []\n",
    "\n",
    "# Utilizar expresiones regulares para encontrar coincidencias de \"zona\" seguida de un número\n",
    "pattern = r'\\bzona\\s+(\\d+)\\b'  # \\b asegura que \"zona\" sea una palabra completa, \\s+ coincide con uno o más espacios, \\d+ coincide con uno o más dígitos\n",
    "\n",
    "# Buscar coincidencias en la columna 'texto' y almacenarlas en la lista\n",
    "for texto in data['rawContent_clean']:\n",
    "    matches = re.findall(pattern, texto)\n",
    "    coincidencias.extend(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contar la frecuencia de cada número\n",
    "conteo_coincidencias = Counter(coincidencias)\n",
    "\n",
    "# Obtener las etiquetas (números) y sus frecuencias\n",
    "etiquetas = list(conteo_coincidencias.keys())\n",
    "frecuencias = list(conteo_coincidencias.values())\n",
    "\n",
    "# Crear el gráfico de barras\n",
    "plt.bar(etiquetas, frecuencias)\n",
    "\n",
    "# Agregar etiquetas y título\n",
    "plt.xlabel('Número')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.title('Frecuencia de coincidencias por número')\n",
    "\n",
    "# Mostrar el gráfico\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede observar que de los tweets relacionados al tráfico, la mayor cantidad vienen de zona 1, zona 10, zona 11 y zona 18. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usuarios = data['user'].unique()\n",
    "print('Cantidad de usuarios unicos en el dataset: ', len(usuarios))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraer_valor(diccionario):\n",
    "    diccionario = eval(diccionario)\n",
    "    return diccionario[\"username\"]\n",
    "\n",
    "data[\"username\"]=data[\"user\"].apply(extraer_valor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_mas_rt = data.groupby('username')['retweetCount'].sum()\n",
    "tr_count_df = tweets_mas_rt.reset_index()\n",
    "tr_count_df = tr_count_df.rename(columns={'retweetCount': 'count'})\n",
    "tr_count_df = tr_count_df.sort_values(by='count', ascending=False)\n",
    "tr_count_df = tr_count_df.head(10)\n",
    "\n",
    "plt.figure(figsize=(10, 6))  # Tamaño del gráfico\n",
    "plt.bar(tr_count_df['username'], tr_count_df['count'])\n",
    "plt.xlabel('Usuario')\n",
    "plt.ylabel('Conteo de Retweets')\n",
    "plt.title('Conteo de Retweets por Usuario')\n",
    "plt.xticks(rotation=90)  # Rotar las etiquetas del eje x para una mejor visualización\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Como podemos ver el usuario {tr_count_df.iloc[0]['username']} pareciera ser un usuario muy activo en Twitter en cuanto a tweets \\nrelacionados al tráfico. Mucha gente comparte la información que el público. Este hallazgo es interesante ya que asumimos \\nque el mayor referente sobre este rubro sería prensa libre, Amílcar Montejo o algún otro medio de comunicación.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_mas_likes = data.groupby('username')['likeCount'].sum()\n",
    "tr_count_df_likes = tweets_mas_likes.reset_index()\n",
    "tr_count_df_likes = tr_count_df_likes.rename(columns={'likeCount': 'count'})\n",
    "tr_count_df_likes = tr_count_df_likes.sort_values(by='count', ascending=False)\n",
    "tr_count_df_likes = tr_count_df_likes.head(10)\n",
    "\n",
    "plt.figure(figsize=(10, 6))  # Tamaño del gráfico\n",
    "plt.bar(tr_count_df['username'], tr_count_df_likes['count'])\n",
    "plt.xlabel('Usuario')\n",
    "plt.ylabel('Conteo de Retweets')\n",
    "plt.title('Conteo de Retweets por Usuario')\n",
    "plt.xticks(rotation=90)  # Rotar las etiquetas del eje x para una mejor visualización\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frecuencia_usuarios = data['username'].value_counts()\n",
    "\n",
    "usuarios_ordenados = frecuencia_usuarios.sort_values(ascending=False)\n",
    "\n",
    "# Tomar los 10 usuarios más frecuentes\n",
    "top_10_usuarios = usuarios_ordenados.head(10)\n",
    "\n",
    "# Crear un gráfico de barras\n",
    "plt.figure(figsize=(15, 6))  # Tamaño del gráfico\n",
    "top_10_usuarios.plot(kind='bar')\n",
    "plt.xlabel('Usuario')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.title('Frecuencia de Usuarios')\n",
    "plt.xticks(rotation=0)  # Rotar las etiquetas del eje x para una mejor visualización\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el modelo pre-entrenado de BERT para análisis de sentimiento\n",
    "model_name = 'nlptown/bert-base-multilingual-uncased-sentiment'\n",
    "model = BertForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Crear una función para realizar análisis de sentimiento\n",
    "def analyze_sentiment(text):\n",
    "    # Tokenizar el texto y obtener la salida del modelo\n",
    "    inputs = tokenizer(text, return_tensors='pt')\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    # Obtener la predicción de sentimiento\n",
    "    prediction = torch.argmax(outputs.logits, dim=1).item()\n",
    "    \n",
    "    # Definir la escala de sentimiento\n",
    "    sentiment_scale = {\n",
    "        0: 'Muy negativo',\n",
    "        1: 'Negativo',\n",
    "        2: 'Neutral',\n",
    "        3: 'Positivo',\n",
    "        4: 'Muy positivo'\n",
    "    }\n",
    "    \n",
    "    # Obtener la etiqueta de sentimiento\n",
    "    sentiment_label = sentiment_scale[prediction]\n",
    "    \n",
    "    return sentiment_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verificar_sentimiento(dataframe, columna):\n",
    "\n",
    "    for valor in dataframe[columna]:\n",
    "        filas = data[data['username'] == valor]\n",
    "        result = filas['rawContent_clean'].apply(analyze_sentiment)\n",
    "        frecuencia_valores = result.value_counts()\n",
    "        valor_mas_comun = frecuencia_valores.idxmax()\n",
    "        print(\"Los tweets del usuario \", valor, \" son mayormente \", valor_mas_comun)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Los sentimientos de los tweets de los usuarios que tienen más retweets son:\")\n",
    "verificar_sentimiento(tr_count_df, \"username\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Los sentimientos de los tweets de los usuarios que tienen más likes son:\")\n",
    "verificar_sentimiento(tr_count_df_likes, \"username\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Pone comentario sobre la negatividad xd\n",
    "ARREGLAR QUE NO SE VAYA A QUEDAR ASI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palabras = ['calzada la paz', 'zona 5', 'hundimiento']\n",
    "patron = '|'.join(palabras)\n",
    "resultados = data[data['rawContent_lemmatized_text'].str.contains(patron, case=False, na=False)]['rawContent_lemmatized_text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lluvia = data[data['rawContent_lemmatized_text'].str.contains('lluvia', case=False)]\n",
    "df_lluvia.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
